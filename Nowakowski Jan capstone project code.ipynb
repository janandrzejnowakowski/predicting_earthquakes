{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree - capstone project\n",
    "\n",
    "## Predicting earthquakes\n",
    "\n",
    "\n",
    "Forecasting natural disasters is a critical part of the field of earth science, \n",
    "due to the impact of early alarming on decreasing the toll of such catastrophes, \n",
    "both with regards to human life, as well as economic loss. Therefore, the goal\n",
    "of this capstone project was to predict the time to the next earthquake from\n",
    "data gathered in a laboratory setup.\n",
    "\n",
    "This project is an entry for a Kaggle competition that can be viewed here:\n",
    "https://www.kaggle.com/c/LANL-Earthquake-Prediction\n",
    "\n",
    "In order for this notebook to run, the training and the testing data needs\n",
    "to be downloaded from this source:\n",
    "https://www.kaggle.com/c/LANL-Earthquake-Prediction/data\n",
    "\n",
    "The 'data' folder needs to be in the same folder as this notebook. In the data folder,\n",
    "there should be a 'train.csv' file that contains all of the training data, \n",
    "'sample_submission.csv' file that contains names of the segments of the training data, \n",
    "as well as a 'test' folder, that contains all of the test data segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT NOTES:\n",
    "### ___\n",
    "## 1. As this project uses a layer optimized for GPU use, it requires running on a computer with a GPU!\n",
    "## (Only true for the final architecture, i.e. the pure-RNN solution)\n",
    "### ___\n",
    "## 2. The values the the training and validation loss area usually higher than the resulting testing error, most likely due to the content of the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------- IMPORTS -----------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# --------------------------------------------- HELPER FUNCTIONS ------------------------------------------\n",
    "\n",
    "def plot_model_history(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    plt.plot(loss, 'b', label = \"Training mae\")\n",
    "    plt.plot(val_loss, 'r', label = \"Validation mae\")\n",
    "    plt.title(\"Training and validation mae\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_data(x, y, time_to_failure, title):\n",
    "    fig, ax1 = plt.subplots(figsize=(16, 8))\n",
    "    plt.title(title)\n",
    "    plt.plot(x, y, color='r')\n",
    "    ax1.set_ylabel(title, color='r')\n",
    "    ax1.set_xlabel('Experiment time [s]')\n",
    "    plt.legend([title], loc=(0.75, 0.9))\n",
    "    ax2 = ax1.twinx()\n",
    "    plt.plot(x, time_to_failure, color='g')\n",
    "    ax2.set_ylabel('Time to failure [s]', color='g')\n",
    "    plt.legend(['time_to_failure'], loc=(0.75, 0.85))\n",
    "    plt.grid(False)\n",
    "    \n",
    "def get_callbacks(model_name):\n",
    "    return [ModelCheckpoint(f'{model_name}.hdf5', monitor='val_loss', save_best_only=True, save_weights_only=False, period=3),\n",
    "            LearningRateScheduler(lambda x: 1. / (1000. * (x + 1)), verbose=1)]\n",
    "\n",
    "def check_for_gpu():\n",
    "    # confirm TensorFlow sees the GPU\n",
    "    from tensorflow.python.client import device_lib\n",
    "    assert 'GPU' in str(device_lib.list_local_devices()), 'Tensorflow does not see GPU, model will not work!'\n",
    "\n",
    "    # confirm Keras sees the GPU\n",
    "    from keras import backend\n",
    "    assert len(backend.tensorflow_backend._get_available_gpus()) > 0 , 'Keras does not see GPU, model will not work!'\n",
    "    \n",
    "    \n",
    "# ----------------------------------------- DATA PREPROCESSING FUNCTIONS ----------------------------------\n",
    "def get_preprocessed_raw_data(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function splits the one very long input measurement into many (4194 if the full dataset is used) shorter measurements\n",
    "    (150000 datapoints each, the same length as each segment of the test dataset)\n",
    "    In addition, it modifies the dimensionality from 1D to 2D\n",
    "    I did this after some period of trial and error with Conv1D layers,\n",
    "    modifying the dimensionality and using Conv2D layers gave much better results\n",
    "    \"\"\"\n",
    "\n",
    "    step = 150000 # size of training data\n",
    "    columns = 400 # 150000 = 375*400, this number of colums makes the shape of the resulting 2D array as close to square as possible\n",
    "    sets = math.floor(len(data)/step)\n",
    "\n",
    "    x_train = np.zeros(shape=(sets, int(step/columns), columns)) \n",
    "    y_train = np.zeros(shape=(sets))\n",
    "\n",
    "    # cut the data into equally-sized chunks\n",
    "    for i, each in enumerate(range(0, len(data), step)):\n",
    "        if i == sets:\n",
    "            break # handling dataset size that does not divide fully into 150 000 long chunks\n",
    "        print(f'Reshaping set number {i+1}/{sets}', end='\\r')\n",
    "        x_train[i] = np.reshape([e[0] for e in data[each:each+step]], (-1, columns)) # reshape 1D into 2D\n",
    "        y_train[i] = data[each+step-1][1]\n",
    "\n",
    "    print()  \n",
    "    x_train = x_train.reshape(*x_train.shape, 1)\n",
    "    assert x_train.shape == (sets, int(step/columns), columns, 1)\n",
    "    \n",
    "    return x_train, y_train\n",
    "    \n",
    "# the product of n_steps and step_length must be equal to the test data segment length (150000)\n",
    "# global variables, used in two different methods\n",
    "n_steps = 150\n",
    "step_length = 1000\n",
    "assert n_steps*step_length == 150000 , 'Incorrect values of n_steps or step_length'\n",
    "\n",
    "# reshapes the data into desired step_length\n",
    "def get_slices(current, n_steps=n_steps, step_length=step_length, last_index=None):\n",
    "    if last_index == None:\n",
    "        last_index=len(current)\n",
    "    temp = (current[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1))\n",
    "    temp = np.ptp(temp, axis=1) # calculate the peak-to-peak\n",
    "    return temp.reshape(n_steps, 1)\n",
    "\n",
    "def get_preprocessed_statistical_data_generators(data, train_validation_split = 0.2):\n",
    "    \"\"\"\n",
    "    This function returns the training and validation generators required by models 3 and 4.\n",
    "    The data returned is the peak-to-peak value from a slice of size \"step_length\"\n",
    "    \"\"\"\n",
    "    def get_generator(data, n_steps=n_steps, step_length=step_length, min_index=0, max_index=None, batch_size=16):\n",
    "        if max_index is None:\n",
    "            max_index = len(data) - 1\n",
    "\n",
    "        while True:\n",
    "            rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "            samples = np.zeros((batch_size, n_steps, 1))\n",
    "            targets = np.zeros(batch_size, )\n",
    "\n",
    "            for j, row in enumerate(rows):\n",
    "                samples[j] = get_slices(data[:, 0], last_index=row)\n",
    "                targets[j] = data[row - 1, 1]\n",
    "            yield samples, targets\n",
    "\n",
    "    batch_size = 64\n",
    "    validation_set_size = math.floor(len(data) * train_validation_split)\n",
    "\n",
    "    train_gen = get_generator(data, batch_size=batch_size, min_index=validation_set_size + 1)\n",
    "    valid_gen = get_generator(data, batch_size=batch_size, max_index=validation_set_size)\n",
    "    \n",
    "    return train_gen, valid_gen\n",
    "\n",
    "\n",
    "# ------------------------------ HELPER FUNCTIONS FOR CREATING SUBMISSION FILES ---------------------------\n",
    "\n",
    "def save_submission(model_name, raw_data_model = True):\n",
    "    # Load submission file\n",
    "    submission = pd.read_csv('./data/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n",
    "\n",
    "    # Load each test data, create the feature matrix, get numeric prediction\n",
    "    for i, seg_id in enumerate(submission.index):\n",
    "        print(f'Predicting result for segment {i+1}/{len(submission.index)} (seg_id:{seg_id})', end='\\r')\n",
    "        seg = pd.read_csv(f'./data/test/{seg_id}.csv')\n",
    "        current = seg['acoustic_data'].values\n",
    "        # different model types require different data modification, hence the if statement below\n",
    "        if raw_data_model:\n",
    "            prepared = np.reshape(current, (-1, 400))\n",
    "            prepared = prepared.reshape(*prepared.shape, 1)\n",
    "            submission.time_to_failure[i] = model.predict(np.expand_dims(prepared, 0))\n",
    "        else:\n",
    "            submission.time_to_failure[i] = model.predict(np.expand_dims(get_slices(current), 0))\n",
    "\n",
    "    # Save\n",
    "    submission.to_csv(f'submission-{model_name}.csv')    \n",
    "    print(submission.head(10))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading the data. \n",
    "\n",
    "Please keep in mind that, for the ease of use, I am reading only the first  1e8 rows (~15%) of the training csv file.\n",
    "TO ACHIEVE GOOD RESULTS, please comment out the \"nrows=1e8\" parameter.\n",
    "But beware, that the training file is has over 9GB!\n",
    "\"\"\"\n",
    "\n",
    "training_data = pd.read_csv(\"./data/train.csv\", nrows=1e8,\n",
    "                            dtype={\"acoustic_data\": np.float32, \"time_to_failure\": np.float32})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling only every 100th of the datapoints for clarity\n",
    "acoustic_data = training_data['acoustic_data'][::100]\n",
    "time_to_failure = training_data['time_to_failure'][::100]\n",
    "\n",
    "# calculating the x axis values - 4e6 datapoints equal one second\n",
    "measurement_frequency = 4e6\n",
    "x = [each/measurement_frequency for each in range(int(len(training_data)))][::100]\n",
    "\n",
    "plot_data(x, acoustic_data, time_to_failure, 'Acoustic data and time to failure')\n",
    "\n",
    "# very important to delete all that we do not need, due to the sheer size of the training dataset\n",
    "del acoustic_data\n",
    "del time_to_failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating statistics over the dataset with points_to_average slice sizes\n",
    "points_to_average = 1000\n",
    "acoustic_data = np.array(training_data['acoustic_data']).reshape(-1, points_to_average)\n",
    "\n",
    "# visualizing the statistical function I had the best results with, namely peak-to-peak\n",
    "statistics = np.ptp(acoustic_data, axis=1)\n",
    "time_to_failure = training_data['time_to_failure'][::points_to_average]\n",
    "x = [each/4e6 for each in range(len(training_data))][::points_to_average]\n",
    "\n",
    "plot_data(x, statistics, time_to_failure, 'Peak-to-peak')\n",
    "\n",
    "\n",
    "del acoustic_data\n",
    "del time_to_failure\n",
    "del x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for feeding to models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "after visualizing, we only need the values\n",
    "VERY IMPORTANT - if this cell is not run, none of the models will work\n",
    "If not, there will be a TypeError: unhashable type: 'slice'\n",
    "\"\"\"\n",
    "if type(training_data) is not np.ndarray: # this cannot be done more than once, hence the check\n",
    "    training_data = training_data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - CNN fed with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , y_train = get_preprocessed_raw_data(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_v1'\n",
    "model = Sequential()\n",
    "\n",
    "input_shape=(None, *x_train.shape[1:])[1:]\n",
    "filters = 8\n",
    "\n",
    "model.add(Conv2D(filters=filters, kernel_size=4, strides=2, padding='same', \n",
    "    activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=1))\n",
    "model.add(Conv2D(filters=int(2*filters), kernel_size=2, strides=2, padding='same', \n",
    "    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Conv2D(filters=int(4*filters), kernel_size=2, strides=2, padding='same', \n",
    "    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(Conv2D(filters=int(8*filters), kernel_size=2, strides=2, padding='same', \n",
    "    activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(1, activation = 'relu'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Nadam(), loss=\"mae\")\n",
    "callbacks = get_callbacks(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=64,\n",
    "          epochs=20,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(model.history)\n",
    "model.load_weights(f'{model_name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell only creates the submission file \n",
    "\n",
    "Can be completely skipped with no harm\n",
    "\"\"\"\n",
    "\n",
    "save_submission(model_name, raw_data_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - CNN + RNN fed with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # no need to recalculate if already defined \n",
    "    x_train\n",
    "    y_train\n",
    "    print('Re-used x_train and y_train from the previous model')\n",
    "except NameError:\n",
    "    x_train , y_train = get_preprocessed_raw_data(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_v2'\n",
    "model = Sequential()\n",
    "\n",
    "filters = 32\n",
    "\n",
    "input_shape=(None, *x_train.shape[1:])[1:]\n",
    "print(input_shape[:-1])\n",
    "\n",
    "model.add(Conv2D(filters=filters, kernel_size=8, strides=2, padding='same', \n",
    "      activation='relu', input_shape=input_shape, name = '1st'))\n",
    "model.add(Conv2D(filters=int(filters/2), kernel_size=4, strides=2, padding='same', \n",
    "      activation='relu', name = '2nd'))\n",
    "model.add(Conv2D(filters=int(filters/4), kernel_size=2, strides=2, padding='same', \n",
    "      activation='relu', name = '3rd'))\n",
    "conv_shape = model.get_layer('3rd').output_shape[1:]\n",
    "print(conv_shape)\n",
    "model.add(Reshape((conv_shape[0], conv_shape[1] * conv_shape[2])))\n",
    "model.add(CuDNNGRU(int(filters/8)))\n",
    "model.add(Dense(int(filters/2), activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'relu'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Nadam(), loss=\"mae\")\n",
    "callbacks = get_callbacks('model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WARNING: this model takes quite a long time to train (longest of all 4)\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          batch_size=16,\n",
    "          epochs=20,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(model.history)\n",
    "model.load_weights(f'{model_name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell only creates the submission file \n",
    "\n",
    "Can be completely skipped with no harm\n",
    "\"\"\"\n",
    "\n",
    "save_submission(model_name, raw_data_model = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - CNN + RNN fed with peak-to-peak data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This model requires having a GPU\n",
    "the method below makes sure that both Keras and Tensorflow see the GPU\n",
    "If this check fails, the model will not work\n",
    "\"\"\"\n",
    "\n",
    "check_for_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this needs to be re-done even if the last model already used it. If not, will cause a StopIteration error.\n",
    "train_gen, valid_gen = get_preprocessed_statistical_data_generators(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_v3'\n",
    "\n",
    "model = Sequential()\n",
    "cells = 64\n",
    "\n",
    "input_shape=(None, 1)\n",
    "\n",
    "model.add(CuDNNGRU(cells, input_shape=input_shape, name = 'rnn'))\n",
    "rnn_shape = model.get_layer('rnn').output_shape[1:]\n",
    "model.add(Reshape((*rnn_shape, 1)))\n",
    "model.add(Conv1D(filters=int(cells/2), kernel_size=8, strides=2, padding='same', \n",
    "      activation='relu'))\n",
    "model.add(Conv1D(filters=int(cells/4), kernel_size=4, strides=2, padding='same', \n",
    "      activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(int(cells/4), activation = 'relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Nadam(), loss=\"mae\")\n",
    "callbacks = get_callbacks(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_gen,\n",
    "                    steps_per_epoch=1000,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_gen,\n",
    "                    validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(model.history)\n",
    "model.load_weights(f'{model_name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell only creates the submission file \n",
    "\n",
    "Can be completely skipped with no harm\n",
    "\"\"\"\n",
    "\n",
    "save_submission(model_name, raw_data_model = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - pure RNN fed with peak-to-peak data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This model requires having a GPU\n",
    "the method below makes sure that both Keras and Tensorflow see the GPU\n",
    "If this check fails, the model will not work\n",
    "\"\"\"\n",
    "\n",
    "check_for_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen, valid_gen = get_preprocessed_statistical_data_generators(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_v4'\n",
    "model = Sequential()\n",
    "\n",
    "model.add(CuDNNGRU(64, input_shape=(None, 1)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Nadam(), loss=\"mae\")\n",
    "callbacks = get_callbacks(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_gen,\n",
    "                    steps_per_epoch=1000,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_gen,\n",
    "                    validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(model.history)\n",
    "model.load_weights(f'{model_name}.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell only creates the submission file \n",
    "\n",
    "Can be completely skipped with no harm\n",
    "\"\"\"\n",
    "\n",
    "save_submission(model_name, raw_data_model = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLGPUENV",
   "language": "python",
   "name": "mlgpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
